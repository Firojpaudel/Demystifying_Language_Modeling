{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsPpTMnqj6kzJszn3aQpS2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/Demystifying_Language_Modeling/blob/main/GPT_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous notebook: [BPE Tokenizer](https://github.com/Firojpaudel/Demystifying_Language_Modeling/blob/main/BPE_Tokenization.ipynb)"
      ],
      "metadata": {
        "id": "g0y2piA_Bkv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the OpenAI paper for [**GPT 2** paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), they have talked about the demerits of vanilla BPE.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src= \"https://i.postimg.cc/vTmDt3ds/Screenshot-2.png\" height=\"375 px\" >\n",
        "  <p> <b><i>SS_1:</i></b><i> Snippet from the paper</i>\n",
        "</div>\n",
        "\n",
        "So yea, they add some complex regex on top of this. The main code is in OpenAI repo: [Click Here](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
        "\n",
        "They have a regex defined:\n",
        "\n",
        "```\n",
        "re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "Concept: *First we breakdown, then we tokenize*"
      ],
      "metadata": {
        "id": "zb7AQr33L_aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "from regex import findall\n",
        "\n",
        "GPT2pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "print(re.findall(GPT2pattern, \"Hello dog! Whats up dog?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UMcep5LNyx6",
        "outputId": "08f96dd4-9733-4050-cfe6-daec4357b7ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ' dog', '!', ' Whats', ' up', ' dog', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Explaining the regex here:\n",
        "\n",
        "- When we encounter `'s or 't or 're or 've or 'm or 'll or 'd`, we spilt them first into seperate token.\n",
        "\n",
        "- Next up: we have: `?\\p{L}+` or `?\\p{N}+`. So this means that when we encounter a text like \"Hello123\"; it gets tokenized as `['Hello', '123']`\n",
        "\n",
        "- Next: We have `?[^\\s\\p{L}\\p{N}]+`. This is for any character that is **not space, letter or a number**. So, that includes all special characters like `!@#.. `.\n",
        "\n",
        "- And finally we have: `\\s+(?!\\S)|\\s+`. Here, `\\s+` matches one or more whitespaces. Then we have negative lookahead `(?!\\S)`. It's used to assert that the \"matched\" whitespace is not followed by non-whitespace character `(\\S)`"
      ],
      "metadata": {
        "id": "9doG4gYIVd1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Test...\n",
        "test1 = \"\"\"\n",
        "for i in range(1, 101):\n",
        "    if i % 3 == 0 and i % 5 == 0:\n",
        "        print(\"FizzBuzz\")\n",
        "    elif i % 3 == 0:\n",
        "        print(\"Fizz\")\n",
        "    elif i % 5 == 0:\n",
        "        print(\"Buzz\")\n",
        "    else:\n",
        "        print(i)\n",
        "\"\"\"\n",
        "br = re.findall(GPT2pattern, test1)\n",
        "print(br)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFhqz3yIq2Bx",
        "outputId": "51d55222-ae26-42dc-a4f9-de7da1b742b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And after this, we can pass into our tokenizer we defined in previous notebok (BPE_Tokenization)"
      ],
      "metadata": {
        "id": "uWVzdovYtgfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str_to_id = {}\n",
        "id_to_str = {}\n",
        "base_str_to_id = {}\n",
        "base_id_to_str = {}\n",
        "vocab = {}\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = findall(GPT2pattern, text)\n",
        "    ids = []\n",
        "    for token in tokens:\n",
        "        if token not in base_str_to_id:\n",
        "            idx = len(base_str_to_id)\n",
        "            base_str_to_id[token] = idx\n",
        "            base_id_to_str[idx] = token\n",
        "        ids.append(base_str_to_id[token])\n",
        "    return ids"
      ],
      "metadata": {
        "id": "zsKg1HCR0mNL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Redefining all that here once again\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if (\n",
        "            i < len(ids) - 1 and\n",
        "            ids[i] == pair[0] and\n",
        "            ids[i+1] == pair[1]\n",
        "        ):\n",
        "            # Safety check: ensure we’re not replacing huge stuff repeatedly\n",
        "            if len(id_to_str[pair[0]] + id_to_str[pair[1]]) > 40:  # or any threshold\n",
        "                newids.append(ids[i])\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids\n",
        "\n",
        "def get_stats(ids):\n",
        "  counts = {}\n",
        "  for pair in zip(ids, ids[1:]):\n",
        "    counts[pair] = counts.get(pair, 0) + 1\n",
        "  return counts\n",
        "\n",
        "\n",
        "merges = {}\n",
        "ids = tokenize(test1 := \"\"\"\n",
        "for i in range(1, 101):\n",
        "    if i % 3 == 0 and i % 5 == 0:\n",
        "        print(\"FizzBuzz\")\n",
        "    elif i % 3 == 0:\n",
        "        print(\"Fizz\")\n",
        "    elif i % 5 == 0:\n",
        "        print(\"Buzz\")\n",
        "    else:\n",
        "        print(i)\n",
        "\"\"\")\n",
        "str_to_id = base_str_to_id.copy()\n",
        "id_to_str = base_id_to_str.copy()\n",
        "\n",
        "num_merges = 20\n",
        "for i in range(num_merges):\n",
        "    stats = get_stats(ids)\n",
        "    if not stats:\n",
        "        break\n",
        "    pair = max(stats, key=stats.get)\n",
        "    idx = max(id_to_str) + 1\n",
        "    merged_token = id_to_str[pair[0]] + id_to_str[pair[1]]\n",
        "\n",
        "    print(f\"merging {pair} → '{merged_token}' into new token {idx}\")\n",
        "\n",
        "    str_to_id[merged_token] = idx\n",
        "    id_to_str[idx] = merged_token\n",
        "    ids = merge(ids, pair, idx)\n",
        "    merges[pair] = idx\n",
        "\n",
        "def encode(text):\n",
        "    tokens = findall(GPT2pattern, text)\n",
        "    ids = [base_str_to_id[token] for token in tokens]\n",
        "\n",
        "    merge_count = 0\n",
        "    max_merges = 20  #! or same as num_merges from training\n",
        "\n",
        "    while merge_count < max_merges:\n",
        "        stats = get_stats(ids)\n",
        "        if not stats:\n",
        "            break\n",
        "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "        if pair not in merges:\n",
        "            break\n",
        "        idx = merges[pair]\n",
        "        ids = merge(ids, pair, idx)\n",
        "        merge_count += 1\n",
        "\n",
        "    return ids\n",
        "\n",
        "def decode(ids):\n",
        "    return \"\".join(id_to_str[idx] for idx in ids)\n",
        "\n",
        "print(\"-------\")\n",
        "print(\"Encoded part:\\n\")\n",
        "encoded_ids = encode(test1)\n",
        "print(encoded_ids)\n",
        "print(\"-------\")\n",
        "print(\"\\nDecoded output:\\n\")\n",
        "print(decode(encoded_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwDLX1WuvFhQ",
        "outputId": "3c4eb26f-8c8b-440c-a1aa-c6b48982478a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging (2, 12) → ' i %' into new token 30\n",
            "merging (14, 15) → ' == 0' into new token 31\n",
            "merging (18, 19) → ':\n",
            "       ' into new token 32\n",
            "merging (32, 20) → ':\n",
            "        print' into new token 33\n",
            "merging (31, 33) → ' == 0:\n",
            "        print' into new token 34\n",
            "merging (34, 21) → ' == 0:\n",
            "        print(\"' into new token 35\n",
            "merging (23, 10) → '\")\n",
            "   ' into new token 36\n",
            "merging (30, 13) → ' i % 3' into new token 37\n",
            "merging (30, 17) → ' i % 5' into new token 38\n",
            "merging (38, 35) → ' i % 5 == 0:\n",
            "        print(\"' into new token 39\n",
            "merging (36, 24) → '\")\n",
            "    elif' into new token 40\n",
            "merging (0, 1) → '\n",
            "for' into new token 41\n",
            "merging (41, 2) → '\n",
            "for i' into new token 42\n",
            "merging (42, 3) → '\n",
            "for i in' into new token 43\n",
            "merging (43, 4) → '\n",
            "for i in range' into new token 44\n",
            "merging (44, 5) → '\n",
            "for i in range(' into new token 45\n",
            "merging (45, 6) → '\n",
            "for i in range(1' into new token 46\n",
            "merging (46, 7) → '\n",
            "for i in range(1,' into new token 47\n",
            "merging (47, 8) → '\n",
            "for i in range(1, 101' into new token 48\n",
            "merging (48, 9) → '\n",
            "for i in range(1, 101):' into new token 49\n",
            "-------\n",
            "Encoded part:\n",
            "\n",
            "[49, 10, 11, 37, 31, 16, 39, 22, 40, 37, 35, 25, 40, 39, 26, 36, 27, 33, 5, 28, 29, 0]\n",
            "-------\n",
            "\n",
            "Decoded output:\n",
            "\n",
            "\n",
            "for i in range(1, 101):\n",
            "    if i % 3 == 0 and i % 5 == 0:\n",
            "        print(\"FizzBuzz\")\n",
            "    elif i % 3 == 0:\n",
            "        print(\"Fizz\")\n",
            "    elif i % 5 == 0:\n",
            "        print(\"Buzz\")\n",
            "    else:\n",
            "        print(i)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now, this is not exactly how OpenAI do it. Infact, they have kept some complex part hidden. However, we can use their library to see the tokenization mechanism.\n"
      ],
      "metadata": {
        "id": "dzobMrnhxdKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq tiktoken"
      ],
      "metadata": {
        "id": "fjVWenRjPdY3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKiFdNzDP5P9",
        "outputId": "b8c870b6-c608-44b6-e957-3be439c66d93"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "for i in range(1, 101):\n",
            "    if i % 3 == 0 and i % 5 == 0:\n",
            "        print(\"FizzBuzz\")\n",
            "    elif i % 3 == 0:\n",
            "        print(\"Fizz\")\n",
            "    elif i % 5 == 0:\n",
            "        print(\"Buzz\")\n",
            "    else:\n",
            "        print(i)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "##@ Testing for GPT-2\n",
        "E = tiktoken.get_encoding(\"gpt2\")\n",
        "ed = E.encode(test1)\n",
        "print(\"-------\")\n",
        "print(ed)\n",
        "print(\"Length of tokens for GPT2 Encoding: \", len(ed))\n",
        "print(\"-------\")\n",
        "dd = E.decode(ed)\n",
        "print(dd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ew4YuVfPiZv",
        "outputId": "1b887598-264f-4f2d-e71a-d7e7cfcd7526"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------\n",
            "[198, 1640, 1312, 287, 2837, 7, 16, 11, 8949, 2599, 198, 220, 220, 220, 611, 1312, 4064, 513, 6624, 657, 290, 1312, 4064, 642, 6624, 657, 25, 198, 220, 220, 220, 220, 220, 220, 220, 3601, 7203, 37, 6457, 48230, 4943, 198, 220, 220, 220, 1288, 361, 1312, 4064, 513, 6624, 657, 25, 198, 220, 220, 220, 220, 220, 220, 220, 3601, 7203, 37, 6457, 4943, 198, 220, 220, 220, 1288, 361, 1312, 4064, 642, 6624, 657, 25, 198, 220, 220, 220, 220, 220, 220, 220, 3601, 7203, 48230, 4943, 198, 220, 220, 220, 2073, 25, 198, 220, 220, 220, 220, 220, 220, 220, 3601, 7, 72, 8, 198]\n",
            "Length of tokens for GPT2 Encoding:  109\n",
            "-------\n",
            "\n",
            "for i in range(1, 101):\n",
            "    if i % 3 == 0 and i % 5 == 0:\n",
            "        print(\"FizzBuzz\")\n",
            "    elif i % 3 == 0:\n",
            "        print(\"Fizz\")\n",
            "    elif i % 5 == 0:\n",
            "        print(\"Buzz\")\n",
            "    else:\n",
            "        print(i)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Testing the same with GPT-4\n",
        "E = tiktoken.get_encoding(\"cl100k_base\")\n",
        "ed = E.encode(test1)\n",
        "print(\"-------\")\n",
        "print(ed)\n",
        "print(\"Length of tokens for GPT4 Encoding: \", len(ed))\n",
        "print(\"-------\")\n",
        "dd = E.decode(ed)\n",
        "print(dd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrhii4OJP9qn",
        "outputId": "d4f1d088-a31e-42fc-dcf5-b9e0987d68f6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------\n",
            "[198, 2000, 602, 304, 2134, 7, 16, 11, 220, 4645, 997, 262, 422, 602, 1034, 220, 18, 624, 220, 15, 323, 602, 1034, 220, 20, 624, 220, 15, 512, 286, 1194, 446, 99615, 60573, 1158, 262, 4508, 602, 1034, 220, 18, 624, 220, 15, 512, 286, 1194, 446, 99615, 1158, 262, 4508, 602, 1034, 220, 20, 624, 220, 15, 512, 286, 1194, 446, 60573, 1158, 262, 775, 512, 286, 1194, 1998, 340]\n",
            "Length of tokens for GPT4 Encoding:  72\n",
            "-------\n",
            "\n",
            "for i in range(1, 101):\n",
            "    if i % 3 == 0 and i % 5 == 0:\n",
            "        print(\"FizzBuzz\")\n",
            "    elif i % 3 == 0:\n",
            "        print(\"Fizz\")\n",
            "    elif i % 5 == 0:\n",
            "        print(\"Buzz\")\n",
            "    else:\n",
            "        print(i)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see there is significant decrease of tokens in GPT 4. This is because GPT-4 handles the tabs and spaces more effeciently hence less tokens"
      ],
      "metadata": {
        "id": "1OUAOypMRaBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "BKBhDuqsRx2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnJrrT2UuLMQ",
        "outputId": "d675d60f-623a-405a-d5f9-8943a1fbb7d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-20 09:43:47--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.104.161\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.104.161|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [application/octet-stream]\n",
            "Saving to: ‘vocab.bpe’\n",
            "\n",
            "vocab.bpe           100%[===================>] 445.62K  1.23MB/s    in 0.4s    \n",
            "\n",
            "2025-06-20 09:43:48 (1.23 MB/s) - ‘vocab.bpe’ saved [456318/456318]\n",
            "\n",
            "--2025-06-20 09:43:48--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.104.161\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.104.161|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‘encoder.json’\n",
            "\n",
            "encoder.json        100%[===================>]   1018K  2.28MB/s    in 0.4s    \n",
            "\n",
            "2025-06-20 09:43:48 (2.28 MB/s) - ‘encoder.json’ saved [1042301/1042301]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "So now, lets take a look at official encoder from openAI\n",
        "\"\"\"\n",
        "\n",
        "import os, json\n",
        "\n",
        "with open('encoder.json', 'r') as f:\n",
        "  encoder = json.load(f)"
      ],
      "metadata": {
        "id": "JVH_hizLSNpW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(encoder)"
      ],
      "metadata": {
        "id": "N4Ld1i_LufPA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: \\\n",
        "> In GPT2, there is only one special token. As we scroll to the last we can see `<|endoftext|>` as the only special token"
      ],
      "metadata": {
        "id": "bfODDBvkunMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoder))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7gXibFbu--w",
        "outputId": "c892ecbb-2459-4f95-e2f4-4c0f5aa210e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder[\"<|endoftext|>\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNtXsaiHvCrp",
        "outputId": "dc4ca223-8236-4626-cfb1-bdeab7a6f1fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "59h2NNd5vKhA"
      }
    }
  ]
}