{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrXP2oIPEHKt8qw7lAC22h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/Demystifying_Language_Modeling/blob/main/GPT_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous notebook: [BPE Tokenizer](https://github.com/Firojpaudel/Demystifying_Language_Modeling/blob/main/BPE_Tokenization.ipynb)"
      ],
      "metadata": {
        "id": "g0y2piA_Bkv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the OpenAI paper for [**GPT 2** paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), they have talked about the demerits of vanilla BPE.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src= \"https://i.postimg.cc/vTmDt3ds/Screenshot-2.png\" height=\"375 px\" >\n",
        "  <p> <b><i>SS_1:</i></b><i> Snippet from the paper</i>\n",
        "</div>\n",
        "\n",
        "So yea, they add some complex regex on top of this. The main code is in OpenAI repo: [Click Here](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
        "\n",
        "They have a regex defined:\n",
        "\n",
        "```\n",
        "re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "Concept: *First we breakdown, then we tokenize*"
      ],
      "metadata": {
        "id": "zb7AQr33L_aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "\n",
        "GPT2pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "print(re.findall(GPT2pattern, \"Hello dog! Whats up dog?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UMcep5LNyx6",
        "outputId": "bd8419ef-cd66-4c94-9dc8-9758b146c0eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ' dog', '!', ' Whats', ' up', ' dog', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Explaining the regex here:\n",
        "\n",
        "- When we encounter `'s or 't or 're or 've or 'm or 'll or 'd`, we spilt them first into seperate token.\n",
        "\n",
        "- Next up: we have: `?\\p{L}+` or `?\\p{N}+`. So this means that when we encounter a text like \"Hello123\"; it gets tokenized as `['Hello', '123']`\n",
        "\n",
        "- Next: We have `?[^\\s\\p{L}\\p{N}]+`. This is for any character that is **not space, letter or a number**. So, that includes all special characters like `!@#.. `.\n",
        "\n",
        "- And finally we have: `\\s+(?!\\S)|\\s+`. Here, `\\s+` matches one or more whitespaces. Then we have negative lookahead `(?!\\S)`. It's used to assert that the \"matched\" whitespace is not followed by non-whitespace character `(\\S)`"
      ],
      "metadata": {
        "id": "9doG4gYIVd1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Test...\n",
        "test1 = \"\"\"\n",
        "for i in range(1, 101):\n",
        "    if i % 3 == 0 and i % 5 == 0:\n",
        "        print(\"FizzBuzz\")\n",
        "    elif i % 3 == 0:\n",
        "        print(\"Fizz\")\n",
        "    elif i % 5 == 0:\n",
        "        print(\"Buzz\")\n",
        "    else:\n",
        "        print(i)\n",
        "\"\"\"\n",
        "br = re.findall(GPT2pattern, test1)\n",
        "print(br)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFhqz3yIq2Bx",
        "outputId": "76895104-fa50-4f1a-8aea-d6b8df632059"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And after this, we can pass into our tokenizer we defined in previous notebok (BPE_Tokenization)"
      ],
      "metadata": {
        "id": "uWVzdovYtgfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str_to_id = {}\n",
        "id_to_str = {}\n",
        "base_str_to_id = {}\n",
        "base_id_to_str = {}\n",
        "vocab = {}\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = findall(GPT2pattern, text)\n",
        "    ids = []\n",
        "    for token in tokens:\n",
        "        if token not in base_str_to_id:\n",
        "            idx = len(base_str_to_id)\n",
        "            base_str_to_id[token] = idx\n",
        "            base_id_to_str[idx] = token\n",
        "        ids.append(base_str_to_id[token])\n",
        "    return ids"
      ],
      "metadata": {
        "id": "zsKg1HCR0mNL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Redefining all that here once again\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if (\n",
        "            i < len(ids) - 1 and\n",
        "            ids[i] == pair[0] and\n",
        "            ids[i+1] == pair[1]\n",
        "        ):\n",
        "            # Safety check: ensure we’re not replacing huge stuff repeatedly\n",
        "            if len(id_to_str[pair[0]] + id_to_str[pair[1]]) > 40:  # or any threshold\n",
        "                newids.append(ids[i])\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids\n",
        "\n",
        "def get_stats(ids):\n",
        "  counts = {}\n",
        "  for pair in zip(ids, ids[1:]):\n",
        "    counts[pair] = counts.get(pair, 0) + 1\n",
        "  return counts\n",
        "\n",
        "\n",
        "merges = {}\n",
        "ids = tokenize(test1 := \"\"\"\n",
        "for i in range(1, 101):\n",
        "    if i % 3 == 0 and i % 5 == 0:\n",
        "        print(\"FizzBuzz\")\n",
        "    elif i % 3 == 0:\n",
        "        print(\"Fizz\")\n",
        "    elif i % 5 == 0:\n",
        "        print(\"Buzz\")\n",
        "    else:\n",
        "        print(i)\n",
        "\"\"\")\n",
        "str_to_id = base_str_to_id.copy()\n",
        "id_to_str = base_id_to_str.copy()\n",
        "\n",
        "num_merges = 20\n",
        "for i in range(num_merges):\n",
        "    stats = get_stats(ids)\n",
        "    if not stats:\n",
        "        break\n",
        "    pair = max(stats, key=stats.get)\n",
        "    idx = max(id_to_str) + 1\n",
        "    merged_token = id_to_str[pair[0]] + id_to_str[pair[1]]\n",
        "\n",
        "    print(f\"merging {pair} → '{merged_token}' into new token {idx}\")\n",
        "\n",
        "    str_to_id[merged_token] = idx\n",
        "    id_to_str[idx] = merged_token\n",
        "    ids = merge(ids, pair, idx)\n",
        "    merges[pair] = idx\n",
        "\n",
        "def encode(text):\n",
        "    tokens = findall(GPT2pattern, text)\n",
        "    ids = [base_str_to_id[token] for token in tokens]\n",
        "\n",
        "    merge_count = 0\n",
        "    max_merges = 20  #! or same as num_merges from training\n",
        "\n",
        "    while merge_count < max_merges:\n",
        "        stats = get_stats(ids)\n",
        "        if not stats:\n",
        "            break\n",
        "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "        if pair not in merges:\n",
        "            break\n",
        "        idx = merges[pair]\n",
        "        ids = merge(ids, pair, idx)\n",
        "        merge_count += 1\n",
        "\n",
        "    return ids\n",
        "\n",
        "def decode(ids):\n",
        "    return \"\".join(id_to_str[idx] for idx in ids)\n",
        "\n",
        "print(\"-------\")\n",
        "print(\"Encoded part:\\n\")\n",
        "encoded_ids = encode(test1)\n",
        "print(encoded_ids)\n",
        "print(\"-------\")\n",
        "print(\"\\nDecoded output:\\n\")\n",
        "print(decode(encoded_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwDLX1WuvFhQ",
        "outputId": "bb1e045a-8df0-4677-b005-04bea2e50edf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging (2, 12) → ' i %' into new token 30\n",
            "merging (14, 15) → ' == 0' into new token 31\n",
            "merging (18, 19) → ':\n",
            "       ' into new token 32\n",
            "merging (32, 20) → ':\n",
            "        print' into new token 33\n",
            "merging (31, 33) → ' == 0:\n",
            "        print' into new token 34\n",
            "merging (34, 21) → ' == 0:\n",
            "        print(\"' into new token 35\n",
            "merging (23, 10) → '\")\n",
            "   ' into new token 36\n",
            "merging (30, 13) → ' i % 3' into new token 37\n",
            "merging (30, 17) → ' i % 5' into new token 38\n",
            "merging (38, 35) → ' i % 5 == 0:\n",
            "        print(\"' into new token 39\n",
            "merging (36, 24) → '\")\n",
            "    elif' into new token 40\n",
            "merging (0, 1) → '\n",
            "for' into new token 41\n",
            "merging (41, 2) → '\n",
            "for i' into new token 42\n",
            "merging (42, 3) → '\n",
            "for i in' into new token 43\n",
            "merging (43, 4) → '\n",
            "for i in range' into new token 44\n",
            "merging (44, 5) → '\n",
            "for i in range(' into new token 45\n",
            "merging (45, 6) → '\n",
            "for i in range(1' into new token 46\n",
            "merging (46, 7) → '\n",
            "for i in range(1,' into new token 47\n",
            "merging (47, 8) → '\n",
            "for i in range(1, 101' into new token 48\n",
            "merging (48, 9) → '\n",
            "for i in range(1, 101):' into new token 49\n",
            "-------\n",
            "Encoded part:\n",
            "\n",
            "[49, 10, 11, 37, 31, 16, 39, 22, 40, 37, 35, 25, 40, 39, 26, 36, 27, 33, 5, 28, 29, 0]\n",
            "-------\n",
            "\n",
            "Decoded output:\n",
            "\n",
            "\n",
            "for i in range(1, 101):\n",
            "    if i % 3 == 0 and i % 5 == 0:\n",
            "        print(\"FizzBuzz\")\n",
            "    elif i % 3 == 0:\n",
            "        print(\"Fizz\")\n",
            "    elif i % 5 == 0:\n",
            "        print(\"Buzz\")\n",
            "    else:\n",
            "        print(i)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dzobMrnhxdKi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}