{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNr2ibs07T7AcqXTGUkal93",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/Demystifying_Language_Modeling/blob/main/BPE_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The website to view: https://tiktokenizer.vercel.app/"
      ],
      "metadata": {
        "id": "SdV5FttPNBBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before Diving into sophisticated tokenization, let's first discuss about  the unicode thing.\n",
        "\n",
        "#### What is Unicode?\n",
        "\n",
        "Computers process information as numbers, specifically in binary. To represent text, computers also use numbers. Unicode is a standard that assigns a unique numerical value, called a code point, to every character across different languages and scripts. This allows computers to consistently handle text from various sources.\n",
        "\n",
        "---\n",
        "\n",
        "Let's see how this is carried out:"
      ],
      "metadata": {
        "id": "h7peoqc2SuIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ To get the unicode of a character, we have `ord` in python that gives the order:\n",
        "\n",
        "ord(\"H\")"
      ],
      "metadata": {
        "id": "jzcW4ZTUWWBb",
        "outputId": "21065187-8bd1-4c9d-a545-14e601948299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##@ And ord doesnot take in String. So passing into for loop we get:\n",
        "\n",
        "[ord(x) for x in \"Hello this is Unicode Testing for string. Since Ord doesnot take string directly\"]"
      ],
      "metadata": {
        "id": "Li7m7VE3N6Jz",
        "outputId": "76b1ae17-6727-4c2c-9f23-10f8dac75f26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[72,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 85,\n",
              " 110,\n",
              " 105,\n",
              " 99,\n",
              " 111,\n",
              " 100,\n",
              " 101,\n",
              " 32,\n",
              " 84,\n",
              " 101,\n",
              " 115,\n",
              " 116,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 102,\n",
              " 111,\n",
              " 114,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 46,\n",
              " 32,\n",
              " 83,\n",
              " 105,\n",
              " 110,\n",
              " 99,\n",
              " 101,\n",
              " 32,\n",
              " 79,\n",
              " 114,\n",
              " 100,\n",
              " 32,\n",
              " 100,\n",
              " 111,\n",
              " 101,\n",
              " 115,\n",
              " 110,\n",
              " 111,\n",
              " 116,\n",
              " 32,\n",
              " 116,\n",
              " 97,\n",
              " 107,\n",
              " 101,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 100,\n",
              " 105,\n",
              " 114,\n",
              " 101,\n",
              " 99,\n",
              " 116,\n",
              " 108,\n",
              " 121]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Also, in UTF-8 format:\n",
        "list(\"Hello👋! this is Unicode Testing for string. Since `Ord` doesnot take string directly\".encode(\"UTF-8\"))"
      ],
      "metadata": {
        "id": "ZPxnK43xZDSP",
        "outputId": "901686cb-10a9-48d8-a19c-f52abd11b398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[72,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111,\n",
              " 240,\n",
              " 159,\n",
              " 145,\n",
              " 139,\n",
              " 33,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 85,\n",
              " 110,\n",
              " 105,\n",
              " 99,\n",
              " 111,\n",
              " 100,\n",
              " 101,\n",
              " 32,\n",
              " 84,\n",
              " 101,\n",
              " 115,\n",
              " 116,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 102,\n",
              " 111,\n",
              " 114,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 46,\n",
              " 32,\n",
              " 83,\n",
              " 105,\n",
              " 110,\n",
              " 99,\n",
              " 101,\n",
              " 32,\n",
              " 96,\n",
              " 79,\n",
              " 114,\n",
              " 100,\n",
              " 96,\n",
              " 32,\n",
              " 100,\n",
              " 111,\n",
              " 101,\n",
              " 115,\n",
              " 110,\n",
              " 111,\n",
              " 116,\n",
              " 32,\n",
              " 116,\n",
              " 97,\n",
              " 107,\n",
              " 101,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 100,\n",
              " 105,\n",
              " 114,\n",
              " 101,\n",
              " 99,\n",
              " 116,\n",
              " 108,\n",
              " 121]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Okay, so great **if we are getting in numerical forms with Unicode, why even tokenize in LLMs**?\n",
        "\n",
        "The major reason behind not relying solely on Unicode for text representation in LLMs is related to efficiency and handling complex language patterns. While Unicode provides a unique number for every character, working at the character level can be computationally expensive for large language models. Additionally, many characters (like emojis or rare symbols) might appear infrequently, making it difficult for the model to learn meaningful representations. Tokenization addresses this by grouping sequences of characters into meaningful units (tokens), which can represent words, sub-words, or even common character sequences. This reduces the overall vocabulary size the model needs to handle, improves computational efficiency, and can help the model better understand the relationships between words and concepts in the text.\n",
        "\n",
        "Okay so with that, now let's discuss about the **BPE Tokenization**.\n",
        "\n",
        "So what happens in BPE Tokenization?\n",
        "\n",
        "- It's a compression technique, which basically contributes in reducing the tokens size.\n",
        "---\n",
        "\n",
        "**Working**:\n",
        "\n",
        "Let's assume our data to be encoded as: $\\text{aaabdaaabac}$\n",
        "\n",
        "- Now: What we do here is; we replace the byte-pair with a byte thats not used in the data.\n",
        "- Here, the most repeated byte-pair right now is $\\text{aa}$\n",
        "- Replacing that with $\\text{\"Z\"}$, we get: $\\text{ZabdZabac}$\n",
        "- Again we have: $\\text{ab} → \\text{\"Y\"}$, then we get: $\\text{ZYdZYac}$\n",
        "- Next, we have: $\\text{ZY} → \\text{\"X\"}$, so we have: $\\text{XdXac}$\n",
        "---"
      ],
      "metadata": {
        "id": "oFzgM6KgSI9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Starting with the length comparision\n",
        "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
        "tokens = text.encode(\"UTF-8\")\n",
        "tokens = list(map(int, tokens))\n",
        "print('-----------')\n",
        "print(text)\n",
        "print(\"length:\", len(text))\n",
        "print('-----------')\n",
        "print(tokens)\n",
        "print('length:', len(tokens))\n",
        "print('-----------')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xDwSaXT-Af7o",
        "outputId": "263af300-09a4-4db8-c01f-1886171078d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------\n",
            "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
            "length: 533\n",
            "-----------\n",
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "length: 616\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see: the uncode alone would rather increase the tokens count. *(More than the original text)*\n",
        "\n",
        "So, next stop: we work on **BPE**:"
      ],
      "metadata": {
        "id": "fAPHAkigE09S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_status (ids):\n",
        "  counts = {}\n",
        "  for pair in zip(ids, ids[1:]):\n",
        "    counts[pair] = counts.get(pair, 0)+1\n",
        "  return counts\n",
        "\n",
        "status = get_status(tokens)\n",
        "\n",
        "print(status)"
      ],
      "metadata": {
        "id": "bu_SdygDFwu_",
        "outputId": "d9b9bf7b-f252-44d1-d020-5f6a9061c8e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(239, 188): 1, (188, 181): 1, (181, 239): 1, (239, 189): 6, (189, 142): 1, (142, 239): 1, (189, 137): 1, (137, 239): 1, (189, 131): 1, (131, 239): 1, (189, 143): 1, (143, 239): 1, (189, 132): 1, (132, 239): 1, (189, 133): 1, (133, 33): 1, (33, 32): 2, (32, 240): 3, (240, 159): 15, (159, 133): 7, (133, 164): 1, (164, 240): 1, (133, 157): 1, (157, 240): 1, (133, 152): 1, (152, 240): 1, (133, 146): 1, (146, 240): 1, (133, 158): 1, (158, 240): 1, (133, 147): 1, (147, 240): 1, (133, 148): 1, (148, 226): 1, (226, 128): 12, (128, 189): 1, (189, 32): 1, (159, 135): 7, (135, 186): 1, (186, 226): 1, (128, 140): 6, (140, 240): 6, (135, 179): 1, (179, 226): 1, (135, 174): 1, (174, 226): 1, (135, 168): 1, (168, 226): 1, (135, 180): 1, (180, 226): 1, (135, 169): 1, (169, 226): 1, (135, 170): 1, (170, 33): 1, (159, 152): 1, (152, 132): 1, (132, 32): 1, (32, 84): 1, (84, 104): 1, (104, 101): 6, (101, 32): 20, (32, 118): 1, (118, 101): 3, (101, 114): 6, (114, 121): 2, (121, 32): 2, (32, 110): 2, (110, 97): 1, (97, 109): 4, (109, 101): 6, (32, 115): 5, (115, 116): 5, (116, 114): 3, (114, 105): 4, (105, 107): 2, (107, 101): 2, (101, 115): 3, (115, 32): 10, (32, 102): 4, (102, 101): 1, (101, 97): 4, (97, 114): 7, (114, 32): 6, (32, 97): 10, (97, 110): 10, (110, 100): 6, (100, 32): 4, (97, 119): 1, (119, 101): 2, (32, 105): 6, (105, 110): 12, (110, 116): 4, (116, 111): 3, (111, 32): 3, (32, 116): 9, (116, 104): 8, (32, 104): 1, (114, 116): 3, (116, 115): 3, (32, 111): 4, (111, 102): 3, (102, 32): 2, (32, 112): 3, (112, 114): 2, (114, 111): 2, (111, 103): 2, (103, 114): 2, (114, 97): 2, (109, 109): 2, (114, 115): 3, (32, 119): 4, (119, 111): 1, (111, 114): 6, (114, 108): 1, (108, 100): 1, (100, 119): 1, (119, 105): 1, (105, 100): 2, (100, 101): 5, (101, 46): 1, (46, 32): 3, (32, 87): 1, (87, 101): 1, (97, 108): 2, (108, 108): 3, (108, 32): 3, (32, 107): 1, (107, 110): 1, (110, 111): 2, (111, 119): 1, (119, 32): 1, (111, 117): 4, (117, 103): 1, (103, 104): 2, (104, 116): 2, (116, 32): 6, (32, 226): 1, (128, 156): 1, (156, 115): 1, (115, 117): 2, (117, 112): 2, (112, 112): 2, (112, 111): 2, (32, 85): 4, (85, 110): 4, (110, 105): 4, (105, 99): 4, (99, 111): 4, (111, 100): 4, (101, 226): 2, (128, 157): 1, (157, 32): 1, (110, 32): 5, (117, 114): 1, (115, 111): 1, (102, 116): 2, (116, 119): 1, (119, 97): 1, (114, 101): 3, (32, 40): 1, (40, 119): 1, (119, 104): 2, (104, 97): 4, (97, 116): 3, (116, 101): 4, (101, 118): 2, (32, 109): 3, (110, 115): 2, (115, 226): 1, (128, 148): 1, (148, 108): 1, (108, 105): 2, (32, 117): 1, (117, 115): 5, (115, 105): 1, (110, 103): 6, (103, 32): 4, (119, 99): 1, (99, 104): 1, (114, 95): 1, (95, 116): 1, (102, 111): 2, (103, 115): 1, (115, 44): 4, (44, 32): 5, (32, 114): 2, (105, 103): 1, (116, 63): 1, (63, 41): 1, (41, 46): 1, (32, 66): 1, (66, 117): 1, (117, 116): 1, (32, 99): 2, (99, 97): 2, (32, 98): 3, (98, 101): 2, (97, 98): 1, (98, 115): 1, (114, 117): 1, (115, 101): 1, (101, 44): 1, (32, 100): 3, (100, 105): 2, (105, 118): 1, (118, 105): 1, (104, 111): 2, (115, 97): 1, (100, 45): 1, (45, 112): 1, (112, 97): 1, (97, 103): 1, (103, 101): 1, (32, 83): 1, (83, 116): 1, (116, 97): 2, (100, 97): 2, (114, 100): 1, (112, 108): 2, (108, 117): 1, (105, 116): 2, (100, 111): 2, (111, 122): 1, (122, 101): 1, (101, 110): 3, (108, 101): 3, (101, 109): 1, (110, 110): 1, (110, 101): 1, (101, 120): 1, (120, 101): 1, (101, 112): 2, (111, 116): 1, (109, 111): 1, (97, 32): 1, (32, 108): 1, (116, 116): 1, (116, 108): 1, (116, 105): 4, (105, 109): 1, (109, 105): 1, (103, 46): 1, (32, 73): 1, (73, 32): 1, (111, 110): 2, (110, 226): 1, (128, 153): 2, (153, 116): 1, (98, 108): 1, (108, 97): 1, (105, 108): 1, (102, 105): 1, (111, 108): 1, (104, 105): 1, (109, 121): 1, (121, 115): 1, (105, 111): 2, (32, 101): 1, (32, 51): 1, (51, 48): 1, (48, 32): 1, (32, 121): 1, (121, 101): 1, (97, 102): 1, (153, 115): 1, (110, 99): 1, (99, 101): 1, (112, 116): 1, (110, 46): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eA_X0UpoaoIv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}