{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ4FpS0Zsfc/YdtqBAMLv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/Demystifying_Language_Modeling/blob/main/BPE_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The website to view: https://tiktokenizer.vercel.app/"
      ],
      "metadata": {
        "id": "SdV5FttPNBBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before Diving into sophisticated tokenization, let's first discuss about  the unicode thing.\n",
        "\n",
        "#### What is Unicode?\n",
        "\n",
        "Computers process information as numbers, specifically in binary. To represent text, computers also use numbers. Unicode is a standard that assigns a unique numerical value, called a code point, to every character across different languages and scripts. This allows computers to consistently handle text from various sources.\n",
        "\n",
        "---\n",
        "\n",
        "Let's see how this is carried out:"
      ],
      "metadata": {
        "id": "h7peoqc2SuIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ To get the unicode of a character, we have `ord` in python that gives the order:\n",
        "\n",
        "ord(\"H\")"
      ],
      "metadata": {
        "id": "jzcW4ZTUWWBb",
        "outputId": "21065187-8bd1-4c9d-a545-14e601948299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##@ And ord doesnot take in String. So passing into for loop we get:\n",
        "\n",
        "[ord(x) for x in \"Hello this is Unicode Testing for string. Since Ord doesnot take string directly\"]"
      ],
      "metadata": {
        "id": "Li7m7VE3N6Jz",
        "outputId": "76b1ae17-6727-4c2c-9f23-10f8dac75f26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[72,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 85,\n",
              " 110,\n",
              " 105,\n",
              " 99,\n",
              " 111,\n",
              " 100,\n",
              " 101,\n",
              " 32,\n",
              " 84,\n",
              " 101,\n",
              " 115,\n",
              " 116,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 102,\n",
              " 111,\n",
              " 114,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 46,\n",
              " 32,\n",
              " 83,\n",
              " 105,\n",
              " 110,\n",
              " 99,\n",
              " 101,\n",
              " 32,\n",
              " 79,\n",
              " 114,\n",
              " 100,\n",
              " 32,\n",
              " 100,\n",
              " 111,\n",
              " 101,\n",
              " 115,\n",
              " 110,\n",
              " 111,\n",
              " 116,\n",
              " 32,\n",
              " 116,\n",
              " 97,\n",
              " 107,\n",
              " 101,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 100,\n",
              " 105,\n",
              " 114,\n",
              " 101,\n",
              " 99,\n",
              " 116,\n",
              " 108,\n",
              " 121]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Also, in UTF-8 format:\n",
        "list(\"HelloðŸ‘‹! this is Unicode Testing for string. Since `Ord` doesnot take string directly\".encode(\"UTF-8\"))"
      ],
      "metadata": {
        "id": "ZPxnK43xZDSP",
        "outputId": "901686cb-10a9-48d8-a19c-f52abd11b398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[72,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111,\n",
              " 240,\n",
              " 159,\n",
              " 145,\n",
              " 139,\n",
              " 33,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 85,\n",
              " 110,\n",
              " 105,\n",
              " 99,\n",
              " 111,\n",
              " 100,\n",
              " 101,\n",
              " 32,\n",
              " 84,\n",
              " 101,\n",
              " 115,\n",
              " 116,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 102,\n",
              " 111,\n",
              " 114,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 46,\n",
              " 32,\n",
              " 83,\n",
              " 105,\n",
              " 110,\n",
              " 99,\n",
              " 101,\n",
              " 32,\n",
              " 96,\n",
              " 79,\n",
              " 114,\n",
              " 100,\n",
              " 96,\n",
              " 32,\n",
              " 100,\n",
              " 111,\n",
              " 101,\n",
              " 115,\n",
              " 110,\n",
              " 111,\n",
              " 116,\n",
              " 32,\n",
              " 116,\n",
              " 97,\n",
              " 107,\n",
              " 101,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 100,\n",
              " 105,\n",
              " 114,\n",
              " 101,\n",
              " 99,\n",
              " 116,\n",
              " 108,\n",
              " 121]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Okay, so great **if we are getting in numerical forms with Unicode, why even tokenize in LLMs**?\n",
        "\n",
        "The major reason behind not relying solely on Unicode for text representation in LLMs is related to efficiency and handling complex language patterns. While Unicode provides a unique number for every character, working at the character level can be computationally expensive for large language models. Additionally, many characters (like emojis or rare symbols) might appear infrequently, making it difficult for the model to learn meaningful representations. Tokenization addresses this by grouping sequences of characters into meaningful units (tokens), which can represent words, sub-words, or even common character sequences. This reduces the overall vocabulary size the model needs to handle, improves computational efficiency, and can help the model better understand the relationships between words and concepts in the text."
      ],
      "metadata": {
        "id": "oFzgM6KgSI9z"
      }
    }
  ]
}