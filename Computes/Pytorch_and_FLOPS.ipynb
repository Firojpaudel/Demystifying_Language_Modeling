{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqlt9rqgFxdPGDifZKLSpu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/Demystifying_Language_Modeling/blob/main/Computes/Pytorch_and_FLOPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Floating Point Precisions:\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zGoVhT9urtBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into the tensors, first lets try to learn the GPU architectures as well.\n",
        "\n",
        "Here, we will be comparing two popular GPUs, one in terms of SOTA performance and another in terms of availability.\n",
        "\n",
        "In terms of SOTA performance, we have Nvidia's **H100** GPU. It is buit on Hooper Architecture. And was released on 2022.\n",
        "\n",
        "**Some of its features:**\n",
        "1. It has $80\\text{GB HBM3}$ memory. (**HBM3** is the fourth generation of High Bandwidth Memory technology)\n",
        "2. Bandwidth: $3.35 \\text{ TB/s}$\n",
        "3. Its optimised  for massive models (e.g., LLMs, diffusion models) with high memory and compute demands.\n",
        "\n",
        "> **Note ⓘ** \\\n",
        "> These are not provided in Colab's Free Tier. However Pro Tier has one.\n",
        "\n",
        "\n",
        "Now, lets go to **T4** GPU.\n",
        "\n",
        "**T4** is based on Turing Architecture.\n",
        "\n",
        "1. Release Date: 2018\n",
        "2. Built for energy-efficient inference and lightweight ML workloads in data centers.\n",
        "3. $16\\text{GB GDDR6}$ memory, $320 \\text{ GB/s}$ memory bandwidth, no NVLink\n",
        "\n",
        "---\n",
        "\n",
        "Also, while we are at it, let's compare $\\text{TFLOPs}$ between these two:\n",
        "\n",
        "\n",
        "|Precision | H100 (SXM, 80GB) | T4 (16 GB) |\n",
        "|----------|------------------|------------|\n",
        "|FP32\t|67 TFLOPS\t|8.1 TFLOPS|\n",
        "|FP16\t|989 TFLOPS (Tensor Cores)\t|65 TFLOPS (Tensor Cores)|\n",
        "|BF16\t|989 TFLOPS (Tensor Cores)\t|Not natively supported (emulated via FP16)|\n",
        "|FP8\t|1979 TFLOPS (Transformer Engine)\t|Not supported|\n",
        "\n",
        "\n",
        "\n",
        "> **Takeaway ⩩** \\\n",
        "> H100’s vastly superior TFLOPS across all precisions, especially FP8 and BF16, makes it ideal for SOTA AI workloads, while T4’s modest FP16/FP32 performance suits lighter tasks.\n",
        "---\n",
        "\n",
        "Now that we know a bit about the architectures, lets now understand Tensor Data Types (Precision) —code based :)\n",
        "\n",
        "> **Note ⓘ** \\\n",
        "> Since we wil be using Colab free tier GPU (T4) the entire time, we will be sticking with all the formats that are compatible with this GPU only."
      ],
      "metadata": {
        "id": "tZ7ME0SetoEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "So, first: precision types and perfomances (we will be testing on MatMul)\n"
      ],
      "metadata": {
        "id": "F52xU7VPuTxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! Imports\n",
        "import torch"
      ],
      "metadata": {
        "id": "vkeJY5xV-4c0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##! FP32 - Matmul\n",
        "\n",
        "A = torch.randn(1024, 2048, device=\"cuda\", dtype= torch.float32)\n",
        "B = torch.randn(2048, 1024, device=\"cuda\", dtype= torch.float32)\n",
        "C = torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "id": "ouiCEyHz-g_v",
        "outputId": "069e6d54-a26c-4209-da8b-e5b871b29bce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.float32\n",
            "VRAM Usage: 28.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! FP16 - Matmul\n",
        "\n",
        "A = torch.randn(1024, 2048, device=\"cuda\", dtype= torch.float16)\n",
        "B = torch.randn(2048, 1024, device=\"cuda\", dtype= torch.float16)\n",
        "C = torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "id": "L6IJMhZo_xb-",
        "outputId": "046d66aa-e249-460a-ea5f-2e47eadfdde5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.float16\n",
            "VRAM Usage: 18.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! FP16 - Matmul with AMP\n",
        "\n",
        "A = torch.randn(1024, 2048, device=\"cuda\")\n",
        "B = torch.randn(2048, 1024, device=\"cuda\")\n",
        "with torch.amp.autocast('cuda', dtype= torch.float16):\n",
        "  C = torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "id": "H_QFGo8n_2BP",
        "outputId": "02ea892e-1872-4ddd-d3c3-a89c86a71ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.float16\n",
            "VRAM Usage: 26.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! BF16 - Emulated (for T4)\n",
        "A = torch.randn(1024, 2048, device=\"cuda\")\n",
        "B = torch.randn(2048, 1024, device=\"cuda\")\n",
        "with torch.amp.autocast('cuda', dtype= torch.bfloat16):\n",
        "  C= torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "id": "EMrivEcvAUcS",
        "outputId": "7b1c4c23-b6ce-4df4-904e-cf97e5d8d6e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.bfloat16\n",
            "VRAM Usage: 26.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! SOTA- batched MatMul\n",
        "\n",
        "A = torch.randn(32, 512, 1024, device=\"cuda\") #! For batched multiplication, we must have a 3D tensor\n",
        "B = torch.randn(32, 1024, 512, device=\"cuda\")\n",
        "with torch.amp.autocast('cuda', dtype= torch.bfloat16):\n",
        "  C= torch.bmm(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "id": "UhZtZXeXENU_",
        "outputId": "13b03394-1a01-46d7-9291-d00a7f6b226d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.bfloat16\n",
            "VRAM Usage: 152.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Now that we have compared all the precisions and their VRAM usages, using `fp16` makes more sense, since it consumes less of VRAM.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "acpwG0r6EXTf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLph3lfUFTPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}