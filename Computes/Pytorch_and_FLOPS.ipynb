{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCWLsjvN7IxHauA6Zpnf2g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/Demystifying_Language_Modeling/blob/main/Computes/Pytorch_and_FLOPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Floating Point Precisions:\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zGoVhT9urtBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into the tensors, first lets try to learn the GPU architectures as well.\n",
        "\n",
        "Here, we will be comparing two popular GPUs, one in terms of SOTA performance and another in terms of availability.\n",
        "\n",
        "In terms of SOTA performance, we have Nvidia's **H100** GPU. It is buit on Hooper Architecture. And was released on 2022.\n",
        "\n",
        "**Some of its features:**\n",
        "1. It has $80\\text{GB HBM3}$ memory. (**HBM3** is the fourth generation of High Bandwidth Memory technology)\n",
        "2. Bandwidth: $3.35 \\text{ TB/s}$\n",
        "3. Its optimised  for massive models (e.g., LLMs, diffusion models) with high memory and compute demands.\n",
        "\n",
        "> **Note ⓘ** \\\n",
        "> These are not provided in Colab's Free Tier. However Pro Tier has one.\n",
        "\n",
        "\n",
        "Now, lets go to **T4** GPU.\n",
        "\n",
        "**T4** is based on Turing Architecture.\n",
        "\n",
        "1. Release Date: 2018\n",
        "2. Built for energy-efficient inference and lightweight ML workloads in data centers.\n",
        "3. $16\\text{GB GDDR6}$ memory, $320 \\text{ GB/s}$ memory bandwidth, no NVLink\n",
        "\n",
        "---\n",
        "\n",
        "Also, while we are at it, let's compare $\\text{TFLOPs}$ between these two:\n",
        "\n",
        "\n",
        "|Precision | H100 (SXM, 80GB) | T4 (16 GB) |\n",
        "|----------|------------------|------------|\n",
        "|FP32\t|67 TFLOPS\t|8.1 TFLOPS|\n",
        "|FP16\t|989 TFLOPS (Tensor Cores)\t|65 TFLOPS (Tensor Cores)|\n",
        "|BF16\t|989 TFLOPS (Tensor Cores)\t|Not natively supported (emulated via FP16)|\n",
        "|FP8\t|1979 TFLOPS (Transformer Engine)\t|Not supported|\n",
        "\n",
        "\n",
        "\n",
        "> **Takeaway ⩩** \\\n",
        "> H100’s vastly superior TFLOPS across all precisions, especially FP8 and BF16, makes it ideal for SOTA AI workloads, while T4’s modest FP16/FP32 performance suits lighter tasks.\n",
        "---\n",
        "\n",
        "Now that we know a bit about the architectures, lets now understand Tensor Data Types (Precision) —code based :)\n",
        "\n",
        "> **Note ⓘ** \\\n",
        "> Since we wil be using Colab free tier GPU (T4) the entire time, we will be sticking with all the formats that are compatible with this GPU only."
      ],
      "metadata": {
        "id": "tZ7ME0SetoEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "So, first: precision types and perfomances (we will be testing on MatMul)\n"
      ],
      "metadata": {
        "id": "F52xU7VPuTxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! Imports\n",
        "import torch"
      ],
      "metadata": {
        "id": "vkeJY5xV-4c0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##! FP32 - Matmul\n",
        "\n",
        "A = torch.randn(1024, 2048, device=\"cuda\", dtype= torch.float32)\n",
        "B = torch.randn(2048, 1024, device=\"cuda\", dtype= torch.float32)\n",
        "C = torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouiCEyHz-g_v",
        "outputId": "069e6d54-a26c-4209-da8b-e5b871b29bce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.float32\n",
            "VRAM Usage: 28.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! FP16 - Matmul\n",
        "\n",
        "A = torch.randn(1024, 2048, device=\"cuda\", dtype= torch.float16)\n",
        "B = torch.randn(2048, 1024, device=\"cuda\", dtype= torch.float16)\n",
        "C = torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6IJMhZo_xb-",
        "outputId": "046d66aa-e249-460a-ea5f-2e47eadfdde5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.float16\n",
            "VRAM Usage: 18.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! FP16 - Matmul with AMP\n",
        "\n",
        "A = torch.randn(1024, 2048, device=\"cuda\")\n",
        "B = torch.randn(2048, 1024, device=\"cuda\")\n",
        "with torch.amp.autocast('cuda', dtype= torch.float16):\n",
        "  C = torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_QFGo8n_2BP",
        "outputId": "02ea892e-1872-4ddd-d3c3-a89c86a71ebc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.float16\n",
            "VRAM Usage: 26.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! BF16 - Emulated (for T4)\n",
        "A = torch.randn(1024, 2048, device=\"cuda\")\n",
        "B = torch.randn(2048, 1024, device=\"cuda\")\n",
        "with torch.amp.autocast('cuda', dtype= torch.bfloat16):\n",
        "  C= torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMrivEcvAUcS",
        "outputId": "7b1c4c23-b6ce-4df4-904e-cf97e5d8d6e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.bfloat16\n",
            "VRAM Usage: 26.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! SOTA- batched MatMul\n",
        "\n",
        "A = torch.randn(32, 512, 1024, device=\"cuda\") #! For batched multiplication, we must have a 3D tensor\n",
        "B = torch.randn(32, 1024, 512, device=\"cuda\")\n",
        "with torch.amp.autocast('cuda', dtype= torch.bfloat16):\n",
        "  C= torch.bmm(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhZtZXeXENU_",
        "outputId": "13b03394-1a01-46d7-9291-d00a7f6b226d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.bfloat16\n",
            "VRAM Usage: 152.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Now that we have compared all the precisions and their VRAM usages, using `fp16` makes more sense, since it consumes less of VRAM.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "acpwG0r6EXTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Up-Next — How Big of a Model Can I Fit on My GPU?\n",
        "\n",
        "Let’s say we’re curious and have been wondering:\n",
        "\n",
        "> *“If I want to train a model with 500M, 1B, or 2B parameters, will it fit in 16 GB GPU VRAM?”*\n",
        "\n",
        "To answer that, let's go step-by-step with a practical estimation formula.\n",
        "\n",
        "---\n",
        "\n",
        "#### General Formula (FP16 + AdamW)\n",
        "\n",
        "We approximate total memory usage like this:\n",
        "\n",
        "$$\n",
        "\\text{VRAM (GB)} \\approx\n",
        "\\underbrace{\\text{Param Memory}}_\\text{Weights + Adam states}\n",
        "+\n",
        "\\underbrace{\\text{Activation Memory}}_\\text{Per sample × batch size}\n",
        "+\n",
        "\\underbrace{1.5}_\\text{GB Safety Buffer}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##### 1. Param Memory (in GB)\n",
        "\n",
        "Assuming **FP16 precision** (2 bytes per param), and using **AdamW optimizer** (3 copies of weights: weights + momentum + variance):\n",
        "\n",
        "$$\n",
        "\\text{Param Memory} = \\frac{6 \\times P}{1024}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( P \\) is in **billions of parameters**\n",
        "\n",
        "---\n",
        "\n",
        "##### 2. Activation Memory (in GB)\n",
        "\n",
        "For each sample:\n",
        "\n",
        "$$\n",
        "\\text{Activation per sample} =\n",
        "\\frac{4 \\times L \\times H \\times S \\times 2}{1024^2} \\quad \\text{(in MB)}\n",
        "$$\n",
        "\n",
        "Then multiply by batch size \\( B \\):\n",
        "\n",
        "$$\n",
        "\\text{Activation Memory} =\n",
        "\\frac{8 \\times L \\times H \\times S \\times B}{1024^3}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( L \\): Layers  \n",
        "- \\( H \\): Hidden size  \n",
        "- \\( S \\): Sequence length  \n",
        "- \\( B \\): Batch size\n",
        "\n",
        "---\n",
        "\n",
        "##### Final Formula\n",
        "\n",
        "Combining both:\n",
        "\n",
        "$$\n",
        "\\text{VRAM (GB)} \\approx\n",
        "\\left( \\frac{6 \\times P}{1024} \\right)\n",
        "+\n",
        "\\left( \\frac{8 \\times L \\times H \\times S \\times B}{1024^3} \\right)\n",
        "+\n",
        "1.5\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##### Example_1: 500M Parameter Model\n",
        "\n",
        "Assumptions:\n",
        "- \\( P = 0.5 \\)\n",
        "- \\( L = 24 \\), \\( H = 1024 \\), \\( S = 128 \\), \\( B = 4 \\)\n",
        "\n",
        "**Param Memory:**\n",
        "\n",
        "$$\n",
        "\\frac{6 \\times 0.5}{1024} \\approx 2.93\\ \\text{GB}\n",
        "$$\n",
        "\n",
        "**Activation Memory:**\n",
        "\n",
        "$$\n",
        "\\frac{8 \\times 24 \\times 1024 \\times 128 \\times 4}{1024^3} \\approx 0.09\\ \\text{GB}\n",
        "$$\n",
        "\n",
        "**Total VRAM:**\n",
        "\n",
        "$$\n",
        "\\text{VRAM} \\approx 2.93 + 0.09 + 1.5 = \\boxed{4.52\\ \\text{GB}}\n",
        "$$\n",
        "\n",
        "This fits easily on a 16GB GPU like **T4**!\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "KLph3lfUFTPG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mzn45GA5SPsZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}