{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS5hj21MVTTpyQ1B8DjmZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/Demystifying_Language_Modeling/blob/main/Computes/Pytorch_and_FLOPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Floating Point Precisions:\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zGoVhT9urtBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into the tensors, first lets try to learn the GPU architectures as well.\n",
        "\n",
        "Here, we will be comparing two popular GPUs, one in terms of SOTA performance and another in terms of availability.\n",
        "\n",
        "In terms of SOTA performance, we have Nvidia's **H100** GPU. It is buit on Hooper Architecture. And was released on 2022.\n",
        "\n",
        "**Some of its features:**\n",
        "1. It has $80\\text{GB HBM3}$ memory. (**HBM3** is the fourth generation of High Bandwidth Memory technology)\n",
        "2. Bandwidth: $3.35 \\text{ TB/s}$\n",
        "3. Its optimised  for massive models (e.g., LLMs, diffusion models) with high memory and compute demands.\n",
        "\n",
        "> **Note ⓘ** \\\n",
        "> These are not provided in Colab's Free Tier. However Pro Tier has one.\n",
        "\n",
        "\n",
        "Now, lets go to **T4** GPU.\n",
        "\n",
        "**T4** is based on Turing Architecture.\n",
        "\n",
        "1. Release Date: 2018\n",
        "2. Built for energy-efficient inference and lightweight ML workloads in data centers.\n",
        "3. $16\\text{GB GDDR6}$ memory, $320 \\text{ GB/s}$ memory bandwidth, no NVLink\n",
        "\n",
        "---\n",
        "\n",
        "Also, while we are at it, let's compare $\\text{TFLOPs}$ between these two:\n",
        "\n",
        "\n",
        "|Precision | H100 (SXM, 80GB) | T4 (16 GB) |\n",
        "|----------|------------------|------------|\n",
        "|FP32\t|67 TFLOPS\t|8.1 TFLOPS|\n",
        "|FP16\t|989 TFLOPS (Tensor Cores)\t|65 TFLOPS (Tensor Cores)|\n",
        "|BF16\t|989 TFLOPS (Tensor Cores)\t|Not natively supported (emulated via FP16)|\n",
        "|FP8\t|1979 TFLOPS (Transformer Engine)\t|Not supported|\n",
        "\n",
        "\n",
        "\n",
        "> **Takeaway ⩩** \\\n",
        "> H100’s vastly superior TFLOPS across all precisions, especially FP8 and BF16, makes it ideal for SOTA AI workloads, while T4’s modest FP16/FP32 performance suits lighter tasks.\n",
        "---\n",
        "\n",
        "Now that we know a bit about the architectures, lets now understand Tensor Data Types (Precision) —code based :)\n",
        "\n",
        "> **Note ⓘ** \\\n",
        "> Since we wil be using Colab free tier GPU (T4) the entire time, we will be sticking with all the formats that are compatible with this GPU only."
      ],
      "metadata": {
        "id": "tZ7ME0SetoEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "So, first: precision types and perfomances (we will be testing on MatMul)\n"
      ],
      "metadata": {
        "id": "F52xU7VPuTxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! Imports\n",
        "import torch"
      ],
      "metadata": {
        "id": "vkeJY5xV-4c0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##! FP32 - Matmul\n",
        "\n",
        "A = torch.randn(1024, 2048, device=\"cuda\", dtype= torch.float32)\n",
        "B = torch.randn(2048, 1024, device=\"cuda\", dtype= torch.float32)\n",
        "C = torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouiCEyHz-g_v",
        "outputId": "069e6d54-a26c-4209-da8b-e5b871b29bce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.float32\n",
            "VRAM Usage: 28.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! FP16 - Matmul\n",
        "\n",
        "A = torch.randn(1024, 2048, device=\"cuda\", dtype= torch.float16)\n",
        "B = torch.randn(2048, 1024, device=\"cuda\", dtype= torch.float16)\n",
        "C = torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6IJMhZo_xb-",
        "outputId": "046d66aa-e249-460a-ea5f-2e47eadfdde5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.float16\n",
            "VRAM Usage: 18.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! FP16 - Matmul with AMP\n",
        "\n",
        "A = torch.randn(1024, 2048, device=\"cuda\")\n",
        "B = torch.randn(2048, 1024, device=\"cuda\")\n",
        "with torch.amp.autocast('cuda', dtype= torch.float16):\n",
        "  C = torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_QFGo8n_2BP",
        "outputId": "02ea892e-1872-4ddd-d3c3-a89c86a71ebc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.float16\n",
            "VRAM Usage: 26.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! BF16 - Emulated (for T4)\n",
        "A = torch.randn(1024, 2048, device=\"cuda\")\n",
        "B = torch.randn(2048, 1024, device=\"cuda\")\n",
        "with torch.amp.autocast('cuda', dtype= torch.bfloat16):\n",
        "  C= torch.matmul(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMrivEcvAUcS",
        "outputId": "7b1c4c23-b6ce-4df4-904e-cf97e5d8d6e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.bfloat16\n",
            "VRAM Usage: 26.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##! SOTA- batched MatMul\n",
        "\n",
        "A = torch.randn(32, 512, 1024, device=\"cuda\") #! For batched multiplication, we must have a 3D tensor\n",
        "B = torch.randn(32, 1024, 512, device=\"cuda\")\n",
        "with torch.amp.autocast('cuda', dtype= torch.bfloat16):\n",
        "  C= torch.bmm(A, B)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Ouput type: {C.dtype}\")\n",
        "print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhZtZXeXENU_",
        "outputId": "13b03394-1a01-46d7-9291-d00a7f6b226d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n",
            "Ouput type: torch.bfloat16\n",
            "VRAM Usage: 152.12 MB\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Now that we have compared all the precisions and their VRAM usages, using `fp16` makes more sense, since it consumes less of VRAM.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "acpwG0r6EXTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Up-Next — How Big of a Model Can I Fit on My GPU?\n",
        "\n",
        "Let’s say we’re curious and have been wondering:\n",
        "\n",
        "> *“If I want to train a model with 500M, 1B, or 2B parameters, will it fit in 16 GB GPU VRAM?”*\n",
        "\n",
        "To answer that, let's go step-by-step with a practical estimation formula.\n",
        "\n",
        "---\n",
        "\n",
        "#### Up-Next — How Big of a Model Can I Fit on My GPU?\n",
        "\n",
        "**Practical VRAM Estimation Formula (FP16 + AdamW):**\n",
        "\n",
        "$$\n",
        "\\text{VRAM (GB)} \\approx\n",
        "\\underbrace{\\text{Param Memory}}_\\text{Weights + Adam states}\n",
        "+\n",
        "\\underbrace{\\text{Activation Memory}}_\\text{Per sample × batch size}\n",
        "+\n",
        "\\underbrace{1.5}_\\text{Safety Buffer}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##### 1. Param Memory (in GB)\n",
        "For **FP16 weights** (2 bytes/param) and **AdamW optimizer** (3 state copies: weights + momentum + variance):\n",
        "\n",
        "$$\n",
        "\\text{Param Memory} = \\frac{6 \\times (P \\times 1000)}{1024}\n",
        "$$\n",
        "- $ P $: Parameters in **billions** (e.g., 0.5 for 500M)  \n",
        "- Multiply by 1000 to convert billions → millions  \n",
        "- *Example*: 1B model = $ \\frac{6 \\times 1000}{1024} \\approx 5.86 \\text{ GB} $\n",
        "\n",
        "---\n",
        "\n",
        "##### 2. Activation Memory (in GB)\n",
        "For forward + backward passes:\n",
        "\n",
        "$$\n",
        "\\text{Activation Memory} = \\frac{8 \\times L \\times H \\times S \\times B}{1024^3}\n",
        "$$\n",
        "- $ L $: Layers  \n",
        "- $ H $: Hidden size  \n",
        "- $ S $: Sequence length  \n",
        "- $ B $: Batch size  \n",
        "- *Derivation*: 4 bytes/activation × 2 (fwd/bwd) × tensor dimensions  \n",
        "\n",
        "---\n",
        "\n",
        "##### Final Combined Formula\n",
        "$$\n",
        "\\boxed{\\text{VRAM (GB)} \\approx\n",
        "\\frac{6 \\times (P \\times 1000)}{1024}\n",
        "+\n",
        "\\frac{8 \\times L \\times H \\times S \\times B}{1024^3}\n",
        "+\n",
        "1.5}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##### Verified Examples\n",
        "**500M Parameter Model** ($P=0.5$)\n",
        "- Config: $L=24, H=1024, S=128, B=4$\n",
        "\n",
        "- **Param**: $ \\frac{6 \\times 500}{1024} \\approx 2.93 \\text{ GB} $\n",
        "\n",
        "- **Activation**: $ \\frac{8 \\times 24 \\times 1024 \\times 128 \\times 4}{1024^3} \\approx 0.09 \\text{ GB} $\n",
        "\n",
        "- **Total**: $ 2.93 + 0.09 + 1.5 = \\boxed{4.52 \\text{ GB}} $  \n",
        "\n",
        "> Fits easily on T4 (16GB)\n",
        "\n",
        "**1B Parameter Model** ($P=1$)\n",
        "- Config: $L=24, H=1024, S=128, B=4$\n",
        "\n",
        "- **Param**: $ \\frac{6 \\times 1000}{1024} \\approx 5.86 \\text{ GB} $\n",
        "\n",
        "- **Activation**: $ 0.09 \\text{ GB} $ (same as above)\n",
        "\n",
        "- **Total**: $ 5.86 + 0.09 + 1.5 = \\boxed{7.45 \\text{ GB}} $  \n",
        "\n",
        "> Comfortably fits\n",
        "\n",
        "**2B Parameter Model** ($P=2$)\n",
        "- Config: $L=24, H=2048, S=128, B=4$\n",
        "\n",
        "- **Param**: $ \\frac{6 \\times 2000}{1024} \\approx 11.72 \\text{ GB} $\n",
        "\n",
        "- **Activation**: $ \\frac{8 \\times 24 \\times 2048 \\times 128 \\times 4}{1024^3} \\approx 0.18 \\text{ GB} $\n",
        "\n",
        "- **Total**: $ 11.72 + 0.18 + 1.5 = \\boxed{13.4 \\text{ GB}} $  \n",
        "\n",
        ">  **Fits but borderline** (16GB - 13.4GB = 2.6GB headroom)\n",
        "---\n"
      ],
      "metadata": {
        "id": "KLph3lfUFTPG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xd9lbBOJUIe8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}