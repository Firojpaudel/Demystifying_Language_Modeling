{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzoIN3gPcDuQCuG5rp/3lF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/Demystifying_Language_Modeling/blob/main/Tokenizer/BPE_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The website to view: https://tiktokenizer.vercel.app/"
      ],
      "metadata": {
        "id": "SdV5FttPNBBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before Diving into sophisticated tokenization, let's first discuss about  the unicode thing.\n",
        "\n",
        "#### What is Unicode?\n",
        "\n",
        "Computers process information as numbers, specifically in binary. To represent text, computers also use numbers. Unicode is a standard that assigns a unique numerical value, called a code point, to every character across different languages and scripts. This allows computers to consistently handle text from various sources.\n",
        "\n",
        "---\n",
        "\n",
        "Let's see how this is carried out:"
      ],
      "metadata": {
        "id": "h7peoqc2SuIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ To get the unicode of a character, we have `ord` in python that gives the order:\n",
        "\n",
        "ord(\"H\")"
      ],
      "metadata": {
        "id": "jzcW4ZTUWWBb",
        "outputId": "21065187-8bd1-4c9d-a545-14e601948299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##@ And ord doesnot take in String. So passing into for loop we get:\n",
        "\n",
        "[ord(x) for x in \"Hello this is Unicode Testing for string. Since Ord doesnot take string directly\"]"
      ],
      "metadata": {
        "id": "Li7m7VE3N6Jz",
        "outputId": "76b1ae17-6727-4c2c-9f23-10f8dac75f26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[72,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 85,\n",
              " 110,\n",
              " 105,\n",
              " 99,\n",
              " 111,\n",
              " 100,\n",
              " 101,\n",
              " 32,\n",
              " 84,\n",
              " 101,\n",
              " 115,\n",
              " 116,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 102,\n",
              " 111,\n",
              " 114,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 46,\n",
              " 32,\n",
              " 83,\n",
              " 105,\n",
              " 110,\n",
              " 99,\n",
              " 101,\n",
              " 32,\n",
              " 79,\n",
              " 114,\n",
              " 100,\n",
              " 32,\n",
              " 100,\n",
              " 111,\n",
              " 101,\n",
              " 115,\n",
              " 110,\n",
              " 111,\n",
              " 116,\n",
              " 32,\n",
              " 116,\n",
              " 97,\n",
              " 107,\n",
              " 101,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 100,\n",
              " 105,\n",
              " 114,\n",
              " 101,\n",
              " 99,\n",
              " 116,\n",
              " 108,\n",
              " 121]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Also, in UTF-8 format:\n",
        "list(\"Helloüëã! this is Unicode Testing for string. Since `Ord` doesnot take string directly\".encode(\"UTF-8\"))"
      ],
      "metadata": {
        "id": "ZPxnK43xZDSP",
        "outputId": "901686cb-10a9-48d8-a19c-f52abd11b398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[72,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111,\n",
              " 240,\n",
              " 159,\n",
              " 145,\n",
              " 139,\n",
              " 33,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 105,\n",
              " 115,\n",
              " 32,\n",
              " 85,\n",
              " 110,\n",
              " 105,\n",
              " 99,\n",
              " 111,\n",
              " 100,\n",
              " 101,\n",
              " 32,\n",
              " 84,\n",
              " 101,\n",
              " 115,\n",
              " 116,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 102,\n",
              " 111,\n",
              " 114,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 46,\n",
              " 32,\n",
              " 83,\n",
              " 105,\n",
              " 110,\n",
              " 99,\n",
              " 101,\n",
              " 32,\n",
              " 96,\n",
              " 79,\n",
              " 114,\n",
              " 100,\n",
              " 96,\n",
              " 32,\n",
              " 100,\n",
              " 111,\n",
              " 101,\n",
              " 115,\n",
              " 110,\n",
              " 111,\n",
              " 116,\n",
              " 32,\n",
              " 116,\n",
              " 97,\n",
              " 107,\n",
              " 101,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 100,\n",
              " 105,\n",
              " 114,\n",
              " 101,\n",
              " 99,\n",
              " 116,\n",
              " 108,\n",
              " 121]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Okay, so great **if we are getting in numerical forms with Unicode, why even tokenize in LLMs**?\n",
        "\n",
        "The major reason behind not relying solely on Unicode for text representation in LLMs is related to efficiency and handling complex language patterns. While Unicode provides a unique number for every character, working at the character level can be computationally expensive for large language models. Additionally, many characters (like emojis or rare symbols) might appear infrequently, making it difficult for the model to learn meaningful representations. Tokenization addresses this by grouping sequences of characters into meaningful units (tokens), which can represent words, sub-words, or even common character sequences. This reduces the overall vocabulary size the model needs to handle, improves computational efficiency, and can help the model better understand the relationships between words and concepts in the text.\n",
        "\n",
        "Okay so with that, now let's discuss about the **BPE Tokenization**.\n",
        "\n",
        "So what happens in BPE Tokenization?\n",
        "\n",
        "- It's a compression technique, which basically contributes in reducing the tokens size.\n",
        "---\n",
        "\n",
        "**Working**:\n",
        "\n",
        "Let's assume our data to be encoded as: $\\text{aaabdaaabac}$\n",
        "\n",
        "- Now: What we do here is; we replace the byte-pair with a byte thats not used in the data.\n",
        "- Here, the most repeated byte-pair right now is $\\text{aa}$\n",
        "- Replacing that with $\\text{\"Z\"}$, we get: $\\text{ZabdZabac}$\n",
        "- Again we have: $\\text{ab} ‚Üí \\text{\"Y\"}$, then we get: $\\text{ZYdZYac}$\n",
        "- Next, we have: $\\text{ZY} ‚Üí \\text{\"X\"}$, so we have: $\\text{XdXac}$\n",
        "---"
      ],
      "metadata": {
        "id": "oFzgM6KgSI9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Starting with the length comparision\n",
        "text = \"ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\"\n",
        "tokens = text.encode(\"UTF-8\")\n",
        "tokens = list(map(int, tokens))\n",
        "print('-----------')\n",
        "print(text)\n",
        "print(\"length:\", len(text))\n",
        "print('-----------')\n",
        "print(tokens)\n",
        "print('length:', len(tokens))\n",
        "print('-----------')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xDwSaXT-Af7o",
        "outputId": "87f7f71c-4b7f-49b9-b1dc-08639598d876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------\n",
            "ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\n",
            "length: 533\n",
            "-----------\n",
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "length: 616\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see: the uncode alone would rather increase the tokens count. *(More than the original text)*\n",
        "\n",
        "So, next stop: we work on **BPE**:"
      ],
      "metadata": {
        "id": "fAPHAkigE09S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_status (ids):\n",
        "  counts = {}\n",
        "  for pair in zip(ids, ids[1:]):\n",
        "    counts[pair] = counts.get(pair, 0)+1\n",
        "  return counts\n",
        "\n",
        "status = get_status(tokens)\n",
        "\n",
        "print(status)"
      ],
      "metadata": {
        "id": "bu_SdygDFwu_",
        "outputId": "605c7dea-cbc5-41ea-9254-fc892ea2e735",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(239, 188): 1, (188, 181): 1, (181, 239): 1, (239, 189): 6, (189, 142): 1, (142, 239): 1, (189, 137): 1, (137, 239): 1, (189, 131): 1, (131, 239): 1, (189, 143): 1, (143, 239): 1, (189, 132): 1, (132, 239): 1, (189, 133): 1, (133, 33): 1, (33, 32): 2, (32, 240): 3, (240, 159): 15, (159, 133): 7, (133, 164): 1, (164, 240): 1, (133, 157): 1, (157, 240): 1, (133, 152): 1, (152, 240): 1, (133, 146): 1, (146, 240): 1, (133, 158): 1, (158, 240): 1, (133, 147): 1, (147, 240): 1, (133, 148): 1, (148, 226): 1, (226, 128): 12, (128, 189): 1, (189, 32): 1, (159, 135): 7, (135, 186): 1, (186, 226): 1, (128, 140): 6, (140, 240): 6, (135, 179): 1, (179, 226): 1, (135, 174): 1, (174, 226): 1, (135, 168): 1, (168, 226): 1, (135, 180): 1, (180, 226): 1, (135, 169): 1, (169, 226): 1, (135, 170): 1, (170, 33): 1, (159, 152): 1, (152, 132): 1, (132, 32): 1, (32, 84): 1, (84, 104): 1, (104, 101): 6, (101, 32): 20, (32, 118): 1, (118, 101): 3, (101, 114): 6, (114, 121): 2, (121, 32): 2, (32, 110): 2, (110, 97): 1, (97, 109): 4, (109, 101): 6, (32, 115): 5, (115, 116): 5, (116, 114): 3, (114, 105): 4, (105, 107): 2, (107, 101): 2, (101, 115): 3, (115, 32): 10, (32, 102): 4, (102, 101): 1, (101, 97): 4, (97, 114): 7, (114, 32): 6, (32, 97): 10, (97, 110): 10, (110, 100): 6, (100, 32): 4, (97, 119): 1, (119, 101): 2, (32, 105): 6, (105, 110): 12, (110, 116): 4, (116, 111): 3, (111, 32): 3, (32, 116): 9, (116, 104): 8, (32, 104): 1, (114, 116): 3, (116, 115): 3, (32, 111): 4, (111, 102): 3, (102, 32): 2, (32, 112): 3, (112, 114): 2, (114, 111): 2, (111, 103): 2, (103, 114): 2, (114, 97): 2, (109, 109): 2, (114, 115): 3, (32, 119): 4, (119, 111): 1, (111, 114): 6, (114, 108): 1, (108, 100): 1, (100, 119): 1, (119, 105): 1, (105, 100): 2, (100, 101): 5, (101, 46): 1, (46, 32): 3, (32, 87): 1, (87, 101): 1, (97, 108): 2, (108, 108): 3, (108, 32): 3, (32, 107): 1, (107, 110): 1, (110, 111): 2, (111, 119): 1, (119, 32): 1, (111, 117): 4, (117, 103): 1, (103, 104): 2, (104, 116): 2, (116, 32): 6, (32, 226): 1, (128, 156): 1, (156, 115): 1, (115, 117): 2, (117, 112): 2, (112, 112): 2, (112, 111): 2, (32, 85): 4, (85, 110): 4, (110, 105): 4, (105, 99): 4, (99, 111): 4, (111, 100): 4, (101, 226): 2, (128, 157): 1, (157, 32): 1, (110, 32): 5, (117, 114): 1, (115, 111): 1, (102, 116): 2, (116, 119): 1, (119, 97): 1, (114, 101): 3, (32, 40): 1, (40, 119): 1, (119, 104): 2, (104, 97): 4, (97, 116): 3, (116, 101): 4, (101, 118): 2, (32, 109): 3, (110, 115): 2, (115, 226): 1, (128, 148): 1, (148, 108): 1, (108, 105): 2, (32, 117): 1, (117, 115): 5, (115, 105): 1, (110, 103): 6, (103, 32): 4, (119, 99): 1, (99, 104): 1, (114, 95): 1, (95, 116): 1, (102, 111): 2, (103, 115): 1, (115, 44): 4, (44, 32): 5, (32, 114): 2, (105, 103): 1, (116, 63): 1, (63, 41): 1, (41, 46): 1, (32, 66): 1, (66, 117): 1, (117, 116): 1, (32, 99): 2, (99, 97): 2, (32, 98): 3, (98, 101): 2, (97, 98): 1, (98, 115): 1, (114, 117): 1, (115, 101): 1, (101, 44): 1, (32, 100): 3, (100, 105): 2, (105, 118): 1, (118, 105): 1, (104, 111): 2, (115, 97): 1, (100, 45): 1, (45, 112): 1, (112, 97): 1, (97, 103): 1, (103, 101): 1, (32, 83): 1, (83, 116): 1, (116, 97): 2, (100, 97): 2, (114, 100): 1, (112, 108): 2, (108, 117): 1, (105, 116): 2, (100, 111): 2, (111, 122): 1, (122, 101): 1, (101, 110): 3, (108, 101): 3, (101, 109): 1, (110, 110): 1, (110, 101): 1, (101, 120): 1, (120, 101): 1, (101, 112): 2, (111, 116): 1, (109, 111): 1, (97, 32): 1, (32, 108): 1, (116, 116): 1, (116, 108): 1, (116, 105): 4, (105, 109): 1, (109, 105): 1, (103, 46): 1, (32, 73): 1, (73, 32): 1, (111, 110): 2, (110, 226): 1, (128, 153): 2, (153, 116): 1, (98, 108): 1, (108, 97): 1, (105, 108): 1, (102, 105): 1, (111, 108): 1, (104, 105): 1, (109, 121): 1, (121, 115): 1, (105, 111): 2, (32, 101): 1, (32, 51): 1, (51, 48): 1, (48, 32): 1, (32, 121): 1, (121, 101): 1, (97, 102): 1, (153, 115): 1, (110, 99): 1, (99, 101): 1, (112, 116): 1, (110, 46): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chr(226), chr(128)"
      ],
      "metadata": {
        "id": "eA_X0UpoaoIv",
        "outputId": "ff102445-da25-4f17-9734-1eef2ee02e84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('√¢', '\\x80')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Also to identify the most common pairs:\n",
        "h= sorted(((v,k) for k,v in status.items()), reverse = True)\n",
        "\n",
        "print(h)"
      ],
      "metadata": {
        "id": "CrariMI3-5CH",
        "outputId": "35c37851-1e66-41c2-e525-b5a9b7a87303",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chr(101), chr(32)"
      ],
      "metadata": {
        "id": "OH0eKVNYBAWj",
        "outputId": "4a8bb334-017a-4251-a41b-3fadab6ceac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('e', ' ')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So that means the text we used above has lot of words ending with \"e\"."
      ],
      "metadata": {
        "id": "KMZrMaLPBdGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##! Also we can call the top pair this way:\n",
        "top_pair = max(status, key= status.get)\n",
        "print(top_pair)"
      ],
      "metadata": {
        "id": "j1xy5MNVB_j7",
        "outputId": "5d961dcb-4759-4b2f-edaa-f1ecc4dbd2db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(101, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Okay, so now that we know the max pair, our next target is to merge them."
      ],
      "metadata": {
        "id": "N8sg04K4CRZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Defining the merging funciton\n",
        "def merge(ids, pair, idx):\n",
        "  #@ iterate through the ids, if we find a pair we swap it out with the **new index\n",
        "  merged = []\n",
        "  i= 0\n",
        "  while i < len(ids):\n",
        "    #@ Checking if the pair matches\n",
        "    if i < len(ids) -1 and ids[i] == pair[0] and ids[i+ 1] == pair[1]:\n",
        "      merged.append(idx)\n",
        "      i += 2 #! Since its on pair ü§∑\n",
        "    else:\n",
        "      merged.append(ids[i])\n",
        "      i += 1\n",
        "  return merged"
      ],
      "metadata": {
        "id": "Pb8JLxZICoJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tkns2 = merge(tokens, top_pair, 256)\n",
        "print(tkns2)\n",
        "\n",
        "print(\"length:\", len(tkns2))\n",
        "print(\"----------\")"
      ],
      "metadata": {
        "id": "eGTr8fS3GMtv",
        "outputId": "906ca501-a99b-4b68-b120-8beb822d5ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "length: 596\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with just that we decreased the count from 616 to 596. That means the $20$ occurences we had is reduced here."
      ],
      "metadata": {
        "id": "N-dcSsCpHBL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"So first we need to know our desired vocabulary size:\n",
        "I'm using 290 for now\"\"\"\n",
        "\n",
        "vocab_size = 290\n",
        "num_merges = vocab_size - 256\n",
        "copy = list(tokens)\n",
        "\n",
        "merges ={}\n",
        "\n",
        "#@ So the real loops starts here:\n",
        "for i in range(num_merges):\n",
        "  status = get_status(copy)\n",
        "  pair = max(status, key = status.get)\n",
        "  indx  = 256 + i\n",
        "  print(f\"Merging {pair} into new token {indx}\")\n",
        "  copy = merge(copy, pair, indx)\n",
        "  merges[pair] = indx"
      ],
      "metadata": {
        "id": "PpaK7j8FHMab",
        "outputId": "e0a1023c-bc11-4622-b541-4390104d2ba1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging (101, 32) into new token 256\n",
            "Merging (240, 159) into new token 257\n",
            "Merging (226, 128) into new token 258\n",
            "Merging (105, 110) into new token 259\n",
            "Merging (115, 32) into new token 260\n",
            "Merging (97, 110) into new token 261\n",
            "Merging (116, 104) into new token 262\n",
            "Merging (257, 133) into new token 263\n",
            "Merging (257, 135) into new token 264\n",
            "Merging (97, 114) into new token 265\n",
            "Merging (239, 189) into new token 266\n",
            "Merging (258, 140) into new token 267\n",
            "Merging (267, 264) into new token 268\n",
            "Merging (101, 114) into new token 269\n",
            "Merging (111, 114) into new token 270\n",
            "Merging (116, 32) into new token 271\n",
            "Merging (259, 103) into new token 272\n",
            "Merging (115, 116) into new token 273\n",
            "Merging (261, 100) into new token 274\n",
            "Merging (32, 262) into new token 275\n",
            "Merging (44, 32) into new token 276\n",
            "Merging (97, 109) into new token 277\n",
            "Merging (275, 256) into new token 278\n",
            "Merging (111, 117) into new token 279\n",
            "Merging (85, 110) into new token 280\n",
            "Merging (280, 105) into new token 281\n",
            "Merging (281, 99) into new token 282\n",
            "Merging (282, 111) into new token 283\n",
            "Merging (283, 100) into new token 284\n",
            "Merging (115, 276) into new token 285\n",
            "Merging (273, 114) into new token 286\n",
            "Merging (101, 265) into new token 287\n",
            "Merging (274, 32) into new token 288\n",
            "Merging (259, 116) into new token 289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-------\")\n",
        "print(\"tokens length:\", len(tokens))\n",
        "print(\"ids length:\", len(copy))\n",
        "print(\"Compression Ratio:\", len(tokens)/len(copy), \"X\")\n",
        "print(\"-------\")"
      ],
      "metadata": {
        "id": "XJeLv3NkL1OU",
        "outputId": "7cc53909-ddd1-421a-ca02-0dfe1328c506",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------\n",
            "tokens length: 616\n",
            "ids length: 398\n",
            "Compression Ratio: 1.5477386934673367 X\n",
            "-------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Okay, so now we dive into **Encoding and Decoding** of these tokens.\n"
      ],
      "metadata": {
        "id": "BJCy8q9bMno2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Okay first \"Encoding\"\n",
        "\n",
        "def encode(text):\n",
        "  \"Given a text: we encode and prepare the tokens\"\n",
        "  tokens = list(text.encode(\"UTF-8\"))\n",
        "  while len(tokens) >= 2:\n",
        "    status = get_status(tokens)\n",
        "    pair = min(status, key= lambda p: merges.get(p, float(\"inf\")))\n",
        "    if pair not in merges:\n",
        "      break\n",
        "    idx = merges[pair]\n",
        "    tokens = merge(tokens, pair, idx)\n",
        "  return tokens\n",
        "\n",
        "encode_test = encode(\"Hey There! üëã\")\n",
        "print(encode_test)"
      ],
      "metadata": {
        "id": "ATrXopX1NHxL",
        "outputId": "067ca021-66cb-4372-fcd0-a413f0bbd4c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[72, 101, 121, 32, 84, 104, 269, 101, 33, 32, 257, 145, 139]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Then we have \"Decoding\" left:\n",
        "\n",
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (p0, p1), idx in merges.items(): #! we just add the merged pairs to the vocabulary\n",
        "  vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "\n",
        "def decode(ids):\n",
        "  \"Given the token ids convert back to string\"\n",
        "  tokens = b\"\".join(vocab[idx] for idx in ids) #! Just working with byte sequences\n",
        "  text = tokens.decode(\"utf-8\", errors=\"replace\") #! If we have a invalid byte sequence, we just relace it with \"?\" instead of raising error.\n",
        "  return text"
      ],
      "metadata": {
        "id": "WC5fjmAz6Iot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode_test))"
      ],
      "metadata": {
        "id": "nO7gEszI7xD2",
        "outputId": "9cb3ab8c-6e57-4baa-d689-49d02583cef2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey There! üëã\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now that we are familiar to vanilla **BPE**, let's move towards some **SOTA** tokenizations used in **GPT Series**\n",
        "\n",
        "The Notebook Link: üëâ[Click Here](https://colab.research.google.com/github/Firojpaudel/Demystifying_Language_Modeling/blob/main/GPT_Tokenizer.ipynb) to redirect.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eQPb31FZBEoF"
      }
    }
  ]
}