{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOys04H0V/qgK0bf/ERFtKs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/Demystifying_Language_Modeling/blob/main/Tokenizer/Tokenizer_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building own GPT-4 Tokenizer\n",
        "---\n"
      ],
      "metadata": {
        "id": "g0y2piA_Bkv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 0: Base Configs"
      ],
      "metadata": {
        "id": "ua56RsL2gjHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "import unicodedata\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "O6euo8G8mZXx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_status(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "    merged = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
        "            merged.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            merged.append(ids[i])\n",
        "            i += 1\n",
        "    return merged\n",
        "\n",
        "def replace_control_characters(s: str) -> str:\n",
        "    chars = []\n",
        "    for ch in s:\n",
        "        if unicodedata.category(ch)[0] != \"C\":\n",
        "            chars.append(ch)\n",
        "        else:\n",
        "            chars.append(f\"\\\\u{ord(ch):04x}\")\n",
        "    return \"\".join(chars)\n",
        "\n",
        "def render_token(t: bytes) -> str:\n",
        "    s = t.decode('utf-8', errors='replace')\n",
        "    s = replace_control_characters(s)\n",
        "    return s\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.merges = {}\n",
        "        self.pattern = \"\"\n",
        "        self.special_tokens = {}\n",
        "        self.vocab = self._build_vocab()\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def encode(self, text):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def decode(self, ids):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _build_vocab(self):\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for (p0, p1), idx in self.merges.items():\n",
        "            vocab[idx] = vocab[p0] + vocab[p1]\n",
        "        for special, idx in self.special_tokens.items():\n",
        "            vocab[idx] = special.encode(\"utf-8\")\n",
        "        return vocab\n",
        "\n",
        "    def save(self, file_prefix):\n",
        "        model_file = file_prefix + \".model\"\n",
        "        with open(model_file, 'w') as f:\n",
        "            f.write(\"minbpe v1\\n\")\n",
        "            f.write(f\"{self.pattern}\\n\")\n",
        "            f.write(f\"{len(self.special_tokens)}\\n\")\n",
        "            for special, idx in self.special_tokens.items():\n",
        "                f.write(f\"{special} {idx}\\n\")\n",
        "            for (idx1, idx2), idx in self.merges.items():\n",
        "                f.write(f\"{idx1} {idx2}\\n\")\n",
        "        vocab_file = file_prefix + \".vocab\"\n",
        "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            for idx, token in self.vocab.items():\n",
        "                s = render_token(token)\n",
        "                if idx in inverted_merges:\n",
        "                    idx0, idx1 = inverted_merges[idx]\n",
        "                    s0 = render_token(self.vocab[idx0])\n",
        "                    s1 = render_token(self.vocab[idx1])\n",
        "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
        "                else:\n",
        "                    f.write(f\"[{s}] {idx}\\n\")\n",
        "\n",
        "    def load(self, model_file):\n",
        "        assert model_file.endswith(\".model\")\n",
        "        merges = {}\n",
        "        special_tokens = {}\n",
        "        idx = 256\n",
        "        with open(model_file, 'r', encoding=\"utf-8\") as f:\n",
        "            version = f.readline().strip()\n",
        "            assert version == \"minbpe v1\"\n",
        "            self.pattern = f.readline().strip()\n",
        "            num_special = int(f.readline().strip())\n",
        "            for _ in range(num_special):\n",
        "                special, special_idx = f.readline().strip().split()\n",
        "                special_tokens[special] = int(special_idx)\n",
        "            for line in f:\n",
        "                idx1, idx2 = map(int, line.split())\n",
        "                merges[(idx1, idx2)] = idx\n",
        "                idx += 1\n",
        "        self.merges = merges\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab = self._build_vocab()\n",
        "\n",
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
      ],
      "metadata": {
        "id": "bK3n9r2MgVG6"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### Step 1: Create a `BasicTokenizer` class that has:\n",
        "- `train`, `encode` and `decode` functions ."
      ],
      "metadata": {
        "id": "2FghkQ2Y4qBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTokenizer(Tokenizer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pattern = GPT4_SPLIT_PATTERN\n",
        "        self.compiled_pattern = re.compile(self.pattern)\n",
        "        self.vocab_idx_to_token = {}\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False):\n",
        "        assert vocab_size >= 0, \"Vocabulary size must be non-negative\"\n",
        "\n",
        "        # Normalize text to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Step 1: Split text into initial tokens\n",
        "        tokens = self.compiled_pattern.findall(text)\n",
        "        if verbose:\n",
        "            print(f\"Initial tokens (sample): {tokens[:20]}\")\n",
        "\n",
        "        # Step 2: Create initial vocabulary, including single characters\n",
        "        unique_tokens = list(dict.fromkeys(tokens))\n",
        "        vocab = {token: idx for idx, token in enumerate(unique_tokens)}\n",
        "        vocab_idx_to_token = {idx: token for idx, token in enumerate(unique_tokens)}\n",
        "\n",
        "        # Add individual characters to vocab for fallback\n",
        "        for token in unique_tokens:\n",
        "            for char in token:\n",
        "                if char not in vocab:\n",
        "                    idx = len(vocab)\n",
        "                    vocab[char] = idx\n",
        "                    vocab_idx_to_token[idx] = char\n",
        "\n",
        "        # Step 3: Convert text to initial IDs\n",
        "        ids = [vocab[token] for token in tokens]\n",
        "\n",
        "        # Step 4: Perform BPE merges\n",
        "        merges = {}\n",
        "        num_merges = vocab_size - len(vocab) if vocab_size > len(vocab) else 0\n",
        "        min_freq = 2\n",
        "        max_token_length = 15  # Tighter constraint\n",
        "\n",
        "        for i in range(num_merges):\n",
        "            stats = get_status(ids)\n",
        "            valid_pairs = {\n",
        "                pair: count for pair, count in stats.items()\n",
        "                if count >= min_freq and len(vocab_idx_to_token[pair[0]] + vocab_idx_to_token[pair[1]]) <= max_token_length\n",
        "            }\n",
        "            if not valid_pairs:\n",
        "                if verbose:\n",
        "                    print(f\"No pairs with frequency >= {min_freq} and length <= {max_token_length}.\")\n",
        "                break\n",
        "            pair = max(valid_pairs, key=valid_pairs.get)\n",
        "            idx = len(vocab)\n",
        "            ids = merge(ids, pair, idx)\n",
        "            merges[pair] = idx\n",
        "\n",
        "            # Update vocabulary\n",
        "            token0 = vocab_idx_to_token[pair[0]]\n",
        "            token1 = vocab_idx_to_token[pair[1]]\n",
        "            new_token = token0 + token1\n",
        "            vocab[new_token] = idx\n",
        "            vocab_idx_to_token[idx] = new_token\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({new_token}) had {stats[pair]} occurrences\")\n",
        "\n",
        "        self.merges = merges\n",
        "        self.vocab = vocab\n",
        "        self.vocab_idx_to_token = vocab_idx_to_token\n",
        "        if verbose:\n",
        "            print(f\"Vocabulary size: {len(vocab)}\")\n",
        "            print(f\"Sample vocab: {dict(list(vocab.items())[:10])}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = text.lower()\n",
        "        tokens = self.compiled_pattern.findall(text)\n",
        "        ids = []\n",
        "        for token in tokens:\n",
        "            if token in self.vocab:\n",
        "                ids.append(self.vocab[token])\n",
        "            else:\n",
        "                # Fallback to longest possible subwords\n",
        "                i = 0\n",
        "                while i < len(token):\n",
        "                    # Try longest matching prefix in vocab\n",
        "                    found = False\n",
        "                    for j in range(len(token), i, -1):\n",
        "                        subword = token[i:j]\n",
        "                        if subword in self.vocab:\n",
        "                            ids.append(self.vocab[subword])\n",
        "                            i = j\n",
        "                            found = True\n",
        "                            break\n",
        "                    if not found:\n",
        "                        # Use single character\n",
        "                        char = token[i]\n",
        "                        if char not in self.vocab:\n",
        "                            idx = len(self.vocab)\n",
        "                            self.vocab[char] = idx\n",
        "                            self.vocab_idx_to_token[idx] = char\n",
        "                        ids.append(self.vocab[char])\n",
        "                        i += 1\n",
        "\n",
        "        # Apply BPE merges\n",
        "        while len(ids) >= 2:\n",
        "            stats = get_status(ids)\n",
        "            # Prioritize earliest merge (lowest merge index)\n",
        "            pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
        "            if pair not in self.merges:\n",
        "                break\n",
        "            idx = self.merges[pair]\n",
        "            ids = merge(ids, pair, idx)\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        tokens = [self.vocab_idx_to_token.get(idx, '\\ufffd') for idx in ids]\n",
        "        return ''.join(tokens)"
      ],
      "metadata": {
        "id": "VX7A5RhF5ERt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "##### Now we test:"
      ],
      "metadata": {
        "id": "FWc6Prs4kzgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training_text = \"\"\"\n",
        "\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "kMFbH2FSr1sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BasicTokenizer()\n",
        "text = (\n",
        "    \"Hello, world! This is a tokenizer drill! ðŸ™Œ \"\n",
        "    \"Namaste! Welcome to the tokenizer. Let's learn more. \"\n",
        "    \"Python is great for building models. Tokenization is fun! \"\n",
        "    \"Hello again! Let's tokenize some text. ðŸ™Œ \"\n",
        ") * 100\n",
        "tokenizer.train(text, vocab_size=500, verbose=True)\n",
        "encoded = tokenizer.encode(\"namaste!\")\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(f\"Original: {text[:50]}...\")\n",
        "print(f\"Encoded: {encoded}\")\n",
        "print(f\"Decoded: {decoded}\")"
      ],
      "metadata": {
        "id": "utuEMfXgnizD",
        "outputId": "756dc93e-97a5-4a12-f18c-b0db1fcdb6af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial tokens (sample): ['hello', ',', ' world', '!', ' this', ' is', ' a', ' tokenizer', ' drill', '!', ' ðŸ™Œ', ' namaste', '!', ' welcome', ' to', ' the', ' tokenizer', '.', ' let', \"'s\"]\n",
            "merge 1/443: (15, 16) -> 57 ( let's) had 200 occurrences\n",
            "merge 2/443: (1, 2) -> 58 (, world) had 100 occurrences\n",
            "merge 3/443: (58, 3) -> 59 (, world!) had 100 occurrences\n",
            "merge 4/443: (59, 4) -> 60 (, world! this) had 100 occurrences\n",
            "merge 5/443: (5, 6) -> 61 ( is a) had 100 occurrences\n",
            "merge 6/443: (61, 7) -> 62 ( is a tokenizer) had 100 occurrences\n",
            "merge 7/443: (8, 3) -> 63 ( drill!) had 100 occurrences\n",
            "merge 8/443: (63, 9) -> 64 ( drill! ðŸ™Œ) had 100 occurrences\n",
            "merge 9/443: (10, 3) -> 65 ( namaste!) had 100 occurrences\n",
            "merge 10/443: (11, 12) -> 66 ( welcome to) had 100 occurrences\n",
            "merge 11/443: (66, 13) -> 67 ( welcome to the) had 100 occurrences\n",
            "merge 12/443: (7, 14) -> 68 ( tokenizer.) had 100 occurrences\n",
            "merge 13/443: (57, 17) -> 69 ( let's learn) had 100 occurrences\n",
            "merge 14/443: (18, 14) -> 70 ( more.) had 100 occurrences\n",
            "merge 15/443: (70, 19) -> 71 ( more. python) had 100 occurrences\n",
            "merge 16/443: (5, 20) -> 72 ( is great) had 100 occurrences\n",
            "merge 17/443: (72, 21) -> 73 ( is great for) had 100 occurrences\n",
            "merge 18/443: (23, 14) -> 74 ( models.) had 100 occurrences\n",
            "merge 19/443: (5, 25) -> 75 ( is fun) had 100 occurrences\n",
            "merge 20/443: (75, 3) -> 76 ( is fun!) had 100 occurrences\n",
            "merge 21/443: (76, 26) -> 77 ( is fun! hello) had 100 occurrences\n",
            "merge 22/443: (27, 3) -> 78 ( again!) had 100 occurrences\n",
            "merge 23/443: (78, 57) -> 79 ( again! let's) had 100 occurrences\n",
            "merge 24/443: (28, 29) -> 80 ( tokenize some) had 100 occurrences\n",
            "merge 25/443: (30, 14) -> 81 ( text.) had 100 occurrences\n",
            "merge 26/443: (81, 9) -> 82 ( text. ðŸ™Œ) had 100 occurrences\n",
            "merge 27/443: (82, 26) -> 83 ( text. ðŸ™Œ hello) had 99 occurrences\n",
            "No pairs with frequency >= 2 and length <= 15.\n",
            "Vocabulary size: 84\n",
            "Sample vocab: {'hello': 0, ',': 1, ' world': 2, '!': 3, ' this': 4, ' is': 5, ' a': 6, ' tokenizer': 7, ' drill': 8, ' ðŸ™Œ': 9}\n",
            "Original: Hello, world! This is a tokenizer drill! ðŸ™Œ Namaste...\n",
            "Encoded: [44, 42, 47, 42, 41, 39, 33, 3]\n",
            "Decoded: namaste!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SGK_Wrpjn2Lj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}